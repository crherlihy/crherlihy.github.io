[{"authors":["Manley Roberts","Himanshu Thakur","Christine Herlihy","Colin White","Samuel Dooley"],"categories":null,"content":"","date":1697428800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697428800,"objectID":"aa3aef4af3914ba3a72a830158fea3c7","permalink":"https://crherlihy.github.io/publication/code_gen/","publishdate":"2023-10-16T00:00:00-04:00","relpermalink":"/publication/code_gen/","section":"publication","summary":"Recent claims about the impressive abilities of large language models (LLMs) are often supported by evaluating publicly available benchmarks. Since LLMs train on wide swaths of the internet, this practice raises concerns of data contamination, i.e., evaluating on examples that are explicitly or implicitly included in the training data. Data contamination remains notoriously challenging to measure and mitigate, even with partial attempts like controlled experimentation of training data, canary strings, or embedding similarities. In this work, we conduct the first thorough longitudinal analysis of data contamination in LLMs by using the natural experiment of training cutoffs in GPT models to look at benchmarks released over time. Specifically, we consider two code/mathematical problem-solving datasets, Codeforces and Project Euler, and find statistically significant trends among LLM pass rate vs. GitHub popularity and release date that provide strong evidence of contamination. By open-sourcing our dataset, raw results, and evaluation framework, our work paves the way for rigorous analyses of data contamination in modern models. We conclude with a discussion of best practices and future steps for publicly releasing benchmarks in the age of LLMs that train on webscale data.","tags":["model evaluation","large language models (LLMs)","GPT","code generation","natural experiment","nlp"],"title":"Data Contamination Through the Lens of Time","type":"publication"},{"authors":["Christine Herlihy","John P. Dickerson"],"categories":null,"content":"","date":1669870800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669870800,"objectID":"5f868ddc4421830f3796eaca738f2c4a","permalink":"https://crherlihy.github.io/publication/nrmab/","publishdate":"2022-12-01T00:00:00-05:00","relpermalink":"/publication/nrmab/","section":"publication","summary":"Restless multi-armed bandits are often used to model budget-constrained resource allocation tasks where receipt of the resource is associated with an increased probability of a favorable state transition. Prior work assumes that individual arms only benefit if they receive the resource directly. However, many allocation tasks occur within communities and can be characterized by positive externalities that allow arms to derive partial benefit when their neighbor(s) receive the resource. We thus introduce networked restless bandits, a novel multi-armed bandit setting in which arms are both restless and embedded within a directed graph. We then present Greta, a graph-aware, Whittle index-based heuristic algorithm that can be used to efficiently construct a constrained reward-maximizing action vector at each timestep. Our empirical results demonstrate that Greta outperforms comparison policies across a range of hyperparameter values and graph topologies.","tags":["restless bandits","resource allocation","sequential decision-making","healthcare","multi-armed bandits","resource allocation over networks","graphs","spillover effects","externalities","python","AAAI 2023"],"title":"Networked Restless Bandits with Positive Externalities","type":"publication"},{"authors":["Christine Herlihy","Aviva Prins","Aravind Srinivasan","John Dickerson"],"categories":null,"content":"","date":1623643200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623643200,"objectID":"77f47eee01b6cb325e3a408e066e772b","permalink":"https://crherlihy.github.io/publication/fair_rmab/","publishdate":"2021-06-14T00:00:00-04:00","relpermalink":"/publication/fair_rmab/","section":"publication","summary":"Restless and collapsing bandits are often used to model budget-constrained resource allocation in settings where arms have action-dependent transition probabilities, such as the allocation of health interventions among patients. However, SOTA Whittle-index-based approaches to this planning problem either do not consider fairness among arms, or incentivize fairness without guaranteeing it. We thus introduce ProbFair, a probabilistically fair policy that maximizes total expected reward and satisfies the budget constraint while ensuring a strictly positive lower bound on the probability of being pulled at each timestep. We evaluate our algorithm on a real-world application, where interventions support continuous positive airway pressure (CPAP) therapy adherence among  patients, as well as on a broader class of synthetic transition matrices. We find that ProbFair preserves utility while providing fairness guarantees.","tags":["restless bandits","algorithmic fairness","resource allocation","sequential decision-making","healthcare","multi-armed bandits","python","KDD 2023"],"title":"Planning to Fairly Allocate: Probabilistic Fairness in the Restless Bandit Setting","type":"publication"},{"authors":["Christine Herlihy","Rachel Rudinger"],"categories":null,"content":"","date":1622606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622606400,"objectID":"db3c8a5ac3fa3eb5dc0d04909d124d35","permalink":"https://crherlihy.github.io/publication/mednli/","publishdate":"2021-06-02T00:00:00-04:00","relpermalink":"/publication/mednli/","section":"publication","summary":"Crowdworker-constructed natural language inference (NLI) datasets have been found to contain statistical artifacts associated with the annotation process that allow hypothesis-only classifiers to achieve better-than-random performance (Poliak et al., 2018; Gururanganet et al., 2018; Tsuchiya, 2018). We investigate whether MedNLI, a physician-annotated dataset with premises extracted from clinical notes, contains such artifacts (Romanov and Shivade, 2018). We find that entailed hypotheses contain generic versions of specific concepts in the premise, as well as modifiers related to responsiveness, duration, and probability. Neutral hypotheses feature conditions and behaviors that co-occur with, or cause, the condition(s) in the premise. Contradiction hypotheses feature explicit negation of the premise and implicit negation via assertion of good health. Adversarial filtering demonstrates that performance degrades when evaluated on the difficult subset. We provide partition information and recommendations for alternative dataset construction strategies for knowledge-intensive domains.","tags":["nlp","annotation artifacts","natural language inference (NLI)","healthcare","adversarial filtering","clinical informatics","python"],"title":"MedNLI Is Not Immune: Natural Language Inference Artifacts in the Clinical Domain","type":"publication"},{"authors":["Micah Halter","Christine Herlihy","James Fairbanks"],"categories":null,"content":"","date":1561953600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561953600,"objectID":"4ed309d46edc9612b12d37332dd76c55","permalink":"https://crherlihy.github.io/publication/semanticmodels/","publishdate":"2019-07-01T00:00:00-04:00","relpermalink":"/publication/semanticmodels/","section":"publication","summary":"Scientists construct and analyze computational models to understand the world. That understanding comes from efforts to augment, combine, and compare models of related phenomena. We propose `SemanticModels.jl`, a system that leverages techniques from static and dynamic program analysis to process executable versions of scientific models to perform such metamodeling tasks. By framing these metamodeling tasks as metaprogramming problems, `SemanticModels.jl` enables writing programs that generate and expand models. To this end, we present a category theory-based framework for defining metamodeling tasks, and extracting semantic information from model implementations, and show how this framework can be used to enhance scientific workflows in a working case study.","tags":["applied category theory","compositional modeling","metamodeling","model generation","model semantics","scientific knowledge mining","scientific computing","julia","ACT 2019"],"title":"A Compositional Framework for Scientific Model Augmentation","type":"publication"},{"authors":null,"categories":null,"content":"The objective of this project is to develop an open-source NLP platform capable of ingesting structured and unstructured patient-level data and notes, and extracting clinically relevant features for the purpose of determining eligibility for clinical trials, constructing reproducible, computational phenotypes, and building patient-level predictive models.\nI am currently focused on integrating supervised and unsupervised machine learning techniques, including non-negative matrix factorization for the purpose of topic modeling, and explore the extent to which real-valued, vector based representations of unstructured texts (e.g., patient notes, clinical trial inclusion criteria; user-generated queries, etc.) can be used for dimesionality reduction and/or standardized evaluation of semantically similar documents.\nDuring the Fall 2018 semester, as a student in CS 8803: Data Analytics Using Deep Learning, I had the opportunity to work with my colleague, classmate (and lead ClarityNLP dev!) Charity Hilton on optimizing portions of this platform. Our final paper outlines our approach and presents our empirical results. We plan to integrate a subset of our improvements soon.\nThis project is under active development, and we host bi-weekly Cooking with ClarityNLP tutorial sessions, which you can join in real-time via webex; Jupyter notebooks and recordings from past sessions are also posted. We welcome questions and suggestions for future sessions via Twitter (use the hashtag #cookingWithClarityNLP), GitHub, or Slack.\n","date":1548046800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548046800,"objectID":"6a622a3580f80a0ab981e5c539018164","permalink":"https://crherlihy.github.io/project/claritynlp/","publishdate":"2019-01-21T00:00:00-05:00","relpermalink":"/project/claritynlp/","section":"project","summary":"An open-source NLP framework for clinical phenotyping.","tags":["machine learning","healthcare","computational phenotyping","feature engineering","nlp","python","graduate coursework"],"title":"ClarityNLP","type":"project"},{"authors":null,"categories":null,"content":"The goal of this DARPA project is to develop an open-source information extraction pipeline that can be used to ingest epidemiology papers and associated source code repositories, extract entities that represent scientific model concepts (e.g., model parameters, functions, causal relationships, etc.), and use these entities to construct a semantic knowledge graph, that can be traversed to create meta-models (e.g., novel combinations/parameterizations of modeling concepts), and/or probablistically reason about how to go from a set of known input(s) to a set of unknown but abstractly identifiable, unitful output(s).\n","date":1461729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461729600,"objectID":"98d00795bbdf0038b2a2d758c717e65c","permalink":"https://crherlihy.github.io/project/aske/","publishdate":"2016-04-27T00:00:00-04:00","relpermalink":"/project/aske/","section":"project","summary":"Extraction and graph-based representation of scientific model components for the purpose of metamodel generation.","tags":["knowledge graph","metamodeling","scientific knowledge","multimodal information","julia"],"title":"Automating Scientific Knowledge Extraction (ASKE)","type":"project"},{"authors":null,"categories":null,"content":"The objective of this GTRI HIVE $25K seed grant project was to develop a decentralized system to facilitate patient-directed provider-to-provider electronic medical record sharing. This required me to solve two related problems: (1) patient matching across EHRs, given the use of inconsistent identifiers; and (2) data transfer in the (potential) absence of provider-to-provider joint database access.\nThe prototype I developed involves representing patients, healthcare providers, and patient-designated stakeholders as nodes on a blockchain network, conducting pairwise cryptographic handshakes, and then allowing them to interact with a smart contract that resembles an escrow account, to facilitate patient-directed provider-to-provider record sharing in the absence of universal patient IDs.\n","date":1461729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461729600,"objectID":"5f671f4644848449db8c2832b46e7ad8","permalink":"https://crherlihy.github.io/project/blockchainforhealthrecords/","publishdate":"2016-04-27T00:00:00-04:00","relpermalink":"/project/blockchainforhealthrecords/","section":"project","summary":"An Ethereum-based prototype platform to facilitate patient-directed provider-to-provider record sharing.","tags":["data privacy","smart contract","blockchain","healthcare","python","ehr"],"title":"Blockchain Platform to Facilitate Patient-Directed Record Sharing","type":"project"},{"authors":null,"categories":null,"content":"I completed this project in conjunction with my graduate coursework as a student in CSE 6250: Big Data Analytics for Healthcare. I worked with my classmates, Charity Hilton and Kausar Mukadam, to develop a PyTorch-based pipeline to use CNNs to detect and localize the 14 thoracic pathologies present in the NIH Chest X-ray dataset.\nOur final paper outlines our approach and presents our empirical results.\n","date":1461729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461729600,"objectID":"8d3615d2767b0df251e8442c80c71661","permalink":"https://crherlihy.github.io/project/chestxray/","publishdate":"2016-04-27T00:00:00-04:00","relpermalink":"/project/chestxray/","section":"project","summary":"PyTorch-based pipeline to use CNNs to detect and localize the 14 thoracic pathologies present in the NIH Chest X-ray dataset.","tags":["machine learning","deep learning","graduate coursework","python","healthcare"],"title":"Chest X-ray Disease Diagnosis with Deep Convolutional Neural Networks","type":"project"}]